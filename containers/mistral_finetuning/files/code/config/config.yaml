paths:
  input_csv_local: "../input/training_database.csv"
  input_csv_sagemaker: "/opt/ml/input/data/training/training_database.csv"
  output_dir_local: "output/finetuned-mistral"
  output_dir_sagemaker: "/opt/ml/model"
  cache_local: "./cache"
  cache_sagemaker: "/opt/ml/cache"
  checkpoint_dir: "/opt/ml/checkpoints"

model:
  name: "mistralai/Mistral-7B-Instruct-v0.2"
  load_in_4bit: true
  compute_dtype: "bfloat16"

lora:
  r: 16
  alpha: 32
  dropout: 0.05
  target_modules:
    [
      "q_proj",
      "k_proj",
      "v_proj",
      "o_proj",
      "gate_proj",
      "up_proj",
      "down_proj",
    ]

tokenization:
  max_input_length: 1024

train:
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  #per_device_eval_batch_size: 30
  gradient_accumulation_steps: 16
  learning_rate: 1.5e-4
  num_train_epochs: 3
  weight_decay: 0.01
  warmup_ratio: 0.03
  lr_scheduler_type: "cosine"
  logging_steps: 100
  save_strategy: "epoch"
  save_steps: 100
  save_total_limit: 3
  evaluation_strategy: "epoch"
  eval_steps: 100
  eval_accumulation_steps: 8
  load_best_model_at_end: true
  resume_from_checkpoint: true
  bf16: true
  gradient_checkpointing: true
  seed: 42
  metric_for_best_model: "eval_avg_macro_f1"
  greater_is_better: true
  remove_unused_columns: False
  dataloader_num_workers: 2
  dataloader_pin_memory: False
  #dataloader_drop_last: True
  dataloader_drop_last: False
