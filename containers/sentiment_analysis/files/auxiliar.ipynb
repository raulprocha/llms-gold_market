{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6ef7af9",
   "metadata": {},
   "source": [
    "## AWS credentials "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c01123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# === Load environment variables ===\n",
    "load_dotenv()\n",
    "\n",
    "bucket = os.getenv(\"AWS_BUCKET\")\n",
    "prefix = os.getenv(\"AWS_PREFIX\", \"llm_pipeline\")  # default value\n",
    "\n",
    "aws_access_key = os.getenv(\"AWS_ACCESS_KEY_ID\")\n",
    "aws_secret_key = os.getenv(\"AWS_SECRET_ACCESS_KEY\")\n",
    "\n",
    "# Optional: initialize boto3 client safely\n",
    "s3_client = boto3.client(\n",
    "    \"s3\",\n",
    "    aws_access_key_id=aws_access_key,\n",
    "    aws_secret_access_key=aws_secret_key\n",
    ")\n",
    "\n",
    "print(f\"âœ… Connected to S3 bucket: {bucket}/{prefix}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423d199",
   "metadata": {},
   "source": [
    "## Csv to jsonL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9af36062",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'headline', 'symbol', 'name', 'content', 'generated_headline'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "input_csv = 'input/generated_headline-to_finbert.csv'\n",
    "\n",
    "output_jsonl = 'headline.jsonl'\n",
    "\n",
    "df = pd.read_csv(input_csv)\n",
    "df['id'] = df['id'].astype(str)\n",
    "print(df.columns)\n",
    "\n",
    "with open(output_jsonl, 'w', encoding='utf-8') as f:\n",
    "    for record in df.to_dict(orient='records'):\n",
    "        f.write(json.dumps(record, ensure_ascii=False) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c927608c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "filepath ='headline.jsonl'\n",
    "filename = 'headline'\n",
    "with tarfile.open(f'{filename}.tar.gz', \"w:gz\") as tar:\n",
    "    tar.add(filepath, arcname=f'{filename}.jsonl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d521ae",
   "metadata": {},
   "source": [
    "## Upload code files to S3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30f4f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id=api_key_aws, aws_secret_access_key=api_key_secret_aws)\n",
    "\n",
    "#s3.upload_file('process.py', bucket, f'{prefix}/code/process.py')\n",
    "#s3.upload_file('llm_utils.py', bucket, f'{prefix}/code/llm_utils.py')\n",
    "#s3.upload_file('requirements.txt', bucket, f'{prefix}/code/requirements.txt')\n",
    "s3.upload_file('code.tar.gz', bucket, f'{prefix}/code/code.tar.gz')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adbf2a9",
   "metadata": {},
   "source": [
    "## directory to tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b357476",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import os\n",
    "\n",
    "with tarfile.open('code.tar.gz', \"w:gz\") as tar:\n",
    "    for filename in os.listdir('code'):\n",
    "        filepath = os.path.join('code', filename)\n",
    "        tar.add(filepath, arcname=filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cd57f90d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method from_pretrained in module transformers.models.auto.auto_factory:\n",
      "\n",
      "from_pretrained(*model_args, **kwargs) class method of transformers.models.auto.modeling_auto.AutoModelForCausalLM\n",
      "    Instantiate one of the model classes of the library (with a causal language modeling head) from a pretrained model.\n",
      "    \n",
      "    The model class to instantiate is selected based on the `model_type` property of the config object (either\n",
      "    passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's missing, by\n",
      "    falling back to using pattern matching on `pretrained_model_name_or_path`:\n",
      "    \n",
      "        - **bart** -- [`BartForCausalLM`] (BART model)\n",
      "        - **bert** -- [`BertLMHeadModel`] (BERT model)\n",
      "        - **bert-generation** -- [`BertGenerationDecoder`] (Bert Generation model)\n",
      "        - **big_bird** -- [`BigBirdForCausalLM`] (BigBird model)\n",
      "        - **bigbird_pegasus** -- [`BigBirdPegasusForCausalLM`] (BigBird-Pegasus model)\n",
      "        - **biogpt** -- [`BioGptForCausalLM`] (BioGpt model)\n",
      "        - **blenderbot** -- [`BlenderbotForCausalLM`] (Blenderbot model)\n",
      "        - **blenderbot-small** -- [`BlenderbotSmallForCausalLM`] (BlenderbotSmall model)\n",
      "        - **bloom** -- [`BloomForCausalLM`] (BLOOM model)\n",
      "        - **camembert** -- [`CamembertForCausalLM`] (CamemBERT model)\n",
      "        - **code_llama** -- [`LlamaForCausalLM`] (CodeLlama model)\n",
      "        - **codegen** -- [`CodeGenForCausalLM`] (CodeGen model)\n",
      "        - **cpmant** -- [`CpmAntForCausalLM`] (CPM-Ant model)\n",
      "        - **ctrl** -- [`CTRLLMHeadModel`] (CTRL model)\n",
      "        - **data2vec-text** -- [`Data2VecTextForCausalLM`] (Data2VecText model)\n",
      "        - **electra** -- [`ElectraForCausalLM`] (ELECTRA model)\n",
      "        - **ernie** -- [`ErnieForCausalLM`] (ERNIE model)\n",
      "        - **falcon** -- [`FalconForCausalLM`] (Falcon model)\n",
      "        - **fuyu** -- [`FuyuForCausalLM`] (Fuyu model)\n",
      "        - **git** -- [`GitForCausalLM`] (GIT model)\n",
      "        - **gpt-sw3** -- [`GPT2LMHeadModel`] (GPT-Sw3 model)\n",
      "        - **gpt2** -- [`GPT2LMHeadModel`] (OpenAI GPT-2 model)\n",
      "        - **gpt_bigcode** -- [`GPTBigCodeForCausalLM`] (GPTBigCode model)\n",
      "        - **gpt_neo** -- [`GPTNeoForCausalLM`] (GPT Neo model)\n",
      "        - **gpt_neox** -- [`GPTNeoXForCausalLM`] (GPT NeoX model)\n",
      "        - **gpt_neox_japanese** -- [`GPTNeoXJapaneseForCausalLM`] (GPT NeoX Japanese model)\n",
      "        - **gptj** -- [`GPTJForCausalLM`] (GPT-J model)\n",
      "        - **llama** -- [`LlamaForCausalLM`] (LLaMA model)\n",
      "        - **marian** -- [`MarianForCausalLM`] (Marian model)\n",
      "        - **mbart** -- [`MBartForCausalLM`] (mBART model)\n",
      "        - **mega** -- [`MegaForCausalLM`] (MEGA model)\n",
      "        - **megatron-bert** -- [`MegatronBertForCausalLM`] (Megatron-BERT model)\n",
      "        - **mistral** -- [`MistralForCausalLM`] (Mistral model)\n",
      "        - **mixtral** -- [`MixtralForCausalLM`] (Mixtral model)\n",
      "        - **mpt** -- [`MptForCausalLM`] (MPT model)\n",
      "        - **musicgen** -- [`MusicgenForCausalLM`] (MusicGen model)\n",
      "        - **mvp** -- [`MvpForCausalLM`] (MVP model)\n",
      "        - **open-llama** -- [`OpenLlamaForCausalLM`] (OpenLlama model)\n",
      "        - **openai-gpt** -- [`OpenAIGPTLMHeadModel`] (OpenAI GPT model)\n",
      "        - **opt** -- [`OPTForCausalLM`] (OPT model)\n",
      "        - **pegasus** -- [`PegasusForCausalLM`] (Pegasus model)\n",
      "        - **persimmon** -- [`PersimmonForCausalLM`] (Persimmon model)\n",
      "        - **phi** -- [`PhiForCausalLM`] (Phi model)\n",
      "        - **plbart** -- [`PLBartForCausalLM`] (PLBart model)\n",
      "        - **prophetnet** -- [`ProphetNetForCausalLM`] (ProphetNet model)\n",
      "        - **qdqbert** -- [`QDQBertLMHeadModel`] (QDQBert model)\n",
      "        - **qwen2** -- [`Qwen2ForCausalLM`] (Qwen2 model)\n",
      "        - **reformer** -- [`ReformerModelWithLMHead`] (Reformer model)\n",
      "        - **rembert** -- [`RemBertForCausalLM`] (RemBERT model)\n",
      "        - **roberta** -- [`RobertaForCausalLM`] (RoBERTa model)\n",
      "        - **roberta-prelayernorm** -- [`RobertaPreLayerNormForCausalLM`] (RoBERTa-PreLayerNorm model)\n",
      "        - **roc_bert** -- [`RoCBertForCausalLM`] (RoCBert model)\n",
      "        - **roformer** -- [`RoFormerForCausalLM`] (RoFormer model)\n",
      "        - **rwkv** -- [`RwkvForCausalLM`] (RWKV model)\n",
      "        - **speech_to_text_2** -- [`Speech2Text2ForCausalLM`] (Speech2Text2 model)\n",
      "        - **transfo-xl** -- [`TransfoXLLMHeadModel`] (Transformer-XL model)\n",
      "        - **trocr** -- [`TrOCRForCausalLM`] (TrOCR model)\n",
      "        - **whisper** -- [`WhisperForCausalLM`] (Whisper model)\n",
      "        - **xglm** -- [`XGLMForCausalLM`] (XGLM model)\n",
      "        - **xlm** -- [`XLMWithLMHeadModel`] (XLM model)\n",
      "        - **xlm-prophetnet** -- [`XLMProphetNetForCausalLM`] (XLM-ProphetNet model)\n",
      "        - **xlm-roberta** -- [`XLMRobertaForCausalLM`] (XLM-RoBERTa model)\n",
      "        - **xlm-roberta-xl** -- [`XLMRobertaXLForCausalLM`] (XLM-RoBERTa-XL model)\n",
      "        - **xlnet** -- [`XLNetLMHeadModel`] (XLNet model)\n",
      "        - **xmod** -- [`XmodForCausalLM`] (X-MOD model)\n",
      "    \n",
      "    The model is set in evaluation mode by default using `model.eval()` (so for instance, dropout modules are\n",
      "    deactivated). To train the model, you should first set it back in training mode with `model.train()`\n",
      "    \n",
      "    Args:\n",
      "        pretrained_model_name_or_path (`str` or `os.PathLike`):\n",
      "            Can be either:\n",
      "    \n",
      "                - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n",
      "                  Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n",
      "                  user or organization name, like `dbmdz/bert-base-german-cased`.\n",
      "                - A path to a *directory* containing model weights saved using\n",
      "                  [`~PreTrainedModel.save_pretrained`], e.g., `./my_model_directory/`.\n",
      "                - A path or url to a *tensorflow index checkpoint file* (e.g, `./tf_model/model.ckpt.index`). In\n",
      "                  this case, `from_tf` should be set to `True` and a configuration object should be provided as\n",
      "                  `config` argument. This loading path is slower than converting the TensorFlow checkpoint in a\n",
      "                  PyTorch model using the provided conversion scripts and loading the PyTorch model afterwards.\n",
      "        model_args (additional positional arguments, *optional*):\n",
      "            Will be passed along to the underlying model `__init__()` method.\n",
      "        config ([`PretrainedConfig`], *optional*):\n",
      "            Configuration for the model to use instead of an automatically loaded configuration. Configuration can\n",
      "            be automatically loaded when:\n",
      "    \n",
      "                - The model is a model provided by the library (loaded with the *model id* string of a pretrained\n",
      "                  model).\n",
      "                - The model was saved using [`~PreTrainedModel.save_pretrained`] and is reloaded by supplying the\n",
      "                  save directory.\n",
      "                - The model is loaded by supplying a local directory as `pretrained_model_name_or_path` and a\n",
      "                  configuration JSON file named *config.json* is found in the directory.\n",
      "        state_dict (*Dict[str, torch.Tensor]*, *optional*):\n",
      "            A state dictionary to use instead of a state dictionary loaded from saved weights file.\n",
      "    \n",
      "            This option can be used if you want to create a model from a pretrained configuration but load your own\n",
      "            weights. In this case though, you should check if using [`~PreTrainedModel.save_pretrained`] and\n",
      "            [`~PreTrainedModel.from_pretrained`] is not a simpler option.\n",
      "        cache_dir (`str` or `os.PathLike`, *optional*):\n",
      "            Path to a directory in which a downloaded pretrained model configuration should be cached if the\n",
      "            standard cache should not be used.\n",
      "        from_tf (`bool`, *optional*, defaults to `False`):\n",
      "            Load the model weights from a TensorFlow checkpoint save file (see docstring of\n",
      "            `pretrained_model_name_or_path` argument).\n",
      "        force_download (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to force the (re-)download of the model weights and configuration files, overriding the\n",
      "            cached versions if they exist.\n",
      "        resume_download (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to delete incompletely received files. Will attempt to resume the download if such a\n",
      "            file exists.\n",
      "        proxies (`Dict[str, str]`, *optional*):\n",
      "            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n",
      "            'http://hostname': 'foo.bar:4012'}`. The proxies are used on each request.\n",
      "        output_loading_info(`bool`, *optional*, defaults to `False`):\n",
      "            Whether ot not to also return a dictionary containing missing keys, unexpected keys and error messages.\n",
      "        local_files_only(`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to only look at local files (e.g., not try downloading the model).\n",
      "        revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n",
      "            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n",
      "            identifier allowed by git.\n",
      "        trust_remote_code (`bool`, *optional*, defaults to `False`):\n",
      "            Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n",
      "            should only be set to `True` for repositories you trust and in which you have read the code, as it will\n",
      "            execute code present on the Hub on your local machine.\n",
      "        code_revision (`str`, *optional*, defaults to `\"main\"`):\n",
      "            The specific revision to use for the code on the Hub, if the code leaves in a different repository than\n",
      "            the rest of the model. It can be a branch name, a tag name, or a commit id, since we use a git-based\n",
      "            system for storing models and other artifacts on huggingface.co, so `revision` can be any identifier\n",
      "            allowed by git.\n",
      "        kwargs (additional keyword arguments, *optional*):\n",
      "            Can be used to update the configuration object (after it being loaded) and initiate the model (e.g.,\n",
      "            `output_attentions=True`). Behaves differently depending on whether a `config` is provided or\n",
      "            automatically loaded:\n",
      "    \n",
      "                - If a configuration is provided with `config`, `**kwargs` will be directly passed to the\n",
      "                  underlying model's `__init__` method (we assume all relevant updates to the configuration have\n",
      "                  already been done)\n",
      "                - If a configuration is not provided, `kwargs` will be first passed to the configuration class\n",
      "                  initialization function ([`~PretrainedConfig.from_pretrained`]). Each key of `kwargs` that\n",
      "                  corresponds to a configuration attribute will be used to override said attribute with the\n",
      "                  supplied `kwargs` value. Remaining keys that do not correspond to any configuration attribute\n",
      "                  will be passed to the underlying model's `__init__` function.\n",
      "    \n",
      "    Examples:\n",
      "    \n",
      "    ```python\n",
      "    >>> from transformers import AutoConfig, AutoModelForCausalLM\n",
      "    \n",
      "    >>> # Download model and configuration from huggingface.co and cache.\n",
      "    >>> model = AutoModelForCausalLM.from_pretrained(\"bert-base-cased\")\n",
      "    \n",
      "    >>> # Update configuration during loading\n",
      "    >>> model = AutoModelForCausalLM.from_pretrained(\"bert-base-cased\", output_attentions=True)\n",
      "    >>> model.config.output_attentions\n",
      "    True\n",
      "    \n",
      "    >>> # Loading from a TF checkpoint file instead of a PyTorch model (slower)\n",
      "    >>> config = AutoConfig.from_pretrained(\"./tf_model/bert_tf_model_config.json\")\n",
      "    >>> model = AutoModelForCausalLM.from_pretrained(\n",
      "    ...     \"./tf_model/bert_tf_checkpoint.ckpt.index\", from_tf=True, config=config\n",
      "    ... )\n",
      "    ```\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "help(AutoModelForCausalLM.from_pretrained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
